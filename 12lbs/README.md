# Introduction

This project focuses on evaluating the robustness of an image foundation model using the GRIT benchmark, specifically on a localization task (object detection).
The objective is to run inference on a chosen model, [adapting-CLIP](https://github.com/pals-ttic/adapting-CLIP) which is a variation of OpenAI's CLIP model.
Then, I calculate its evaluation metric (mAP), and determine its zero-shot metric on the GRIT detection dataset.

# Environment & Dataset Setup

* Google Colab equipped with an A100 GPU
* GRIT benchmark, including ImageNet, COCO, ADE20K, and Visual Genome (VG) dataset
* adapting-CLIP model, which uses Flickr and Visual Genome (VG) images along with Zero-Shot Generalization (ZSG) annotations.

# Model Inference

[Figures]

# GRIT Benchmark(mAP and zero-shot metric)


[Figures]


# Challenges and Solutions


# Summary


# References

