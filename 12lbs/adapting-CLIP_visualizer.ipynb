{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNWuoughZ1LixB4fldk5jDd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Adapting CLIP model"],"metadata":{"id":"Zx0g1YT8vps_"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuYG3ZEXvgia","executionInfo":{"status":"ok","timestamp":1681655843276,"user_tz":300,"elapsed":39950,"user":{"displayName":"Jay Suh","userId":"06534266656340944442"}},"outputId":"c1811b11-8b41-4bff-e339-b57b5cf2dd5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ftfy\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.65.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy) (0.2.6)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.1.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-g25ioto1\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-g25ioto1\n","  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (4.65.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (2.0.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from clip==1.0) (0.15.1+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from ftfy->clip==1.0) (0.2.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.11.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (2.0.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (3.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->clip==1.0) (1.11.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->clip==1.0) (2.27.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->clip==1.0) (2.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369398 sha256=5506a9ea21ba62480cb8f079bb4c4010aa3e46eb89ccc3790fe0c8b623ab9df8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-n678m51m/wheels/c8/e4/e1/11374c111387672fc2068dfbe0d4b424cb9cdd1b2e184a71b5\n","Successfully built clip\n","Installing collected packages: clip\n","Successfully installed clip-1.0\n","/content/drive/MyDrive/adapting-CLIP\n"]}],"source":["######## Mount the drive ########\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","######## Install the dependencies ########\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git\n","%cd /content/drive/MyDrive/adapting-CLIP\n","#%cd ../../adapting-CLIP"]},{"cell_type":"code","source":["#import argparse\n","#import os.path as osp\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","from models.slic_vit import SLICViT\n","from models.ss_baseline import SSBaseline\n","from models.resnet_high_res import ResNetHighRes\n","from utils.zsg_data import FlickrDataset, VGDataset\n","from utils.grounding_evaluator import GroundingEvaluator\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import cv2\n","import random\n","%matplotlib inline"],"metadata":{"id":"VQFvAPnYv0DS","executionInfo":{"status":"ok","timestamp":1681655862239,"user_tz":300,"elapsed":18967,"user":{"displayName":"Jay Suh","userId":"06534266656340944442"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# ! python eval.py --model vit14 --dataset flickr_s1_val --iou_thr 0.5 --num_samples 500\n","\n","model = SLICViT\n","args = {\n","    'model': 'vit14',\n","    'alpha': 0.75,\n","    'aggregation': 'mean',\n","    'n_segments': list(range(100, 601, 50)),\n","    'temperature': 0.02,\n","    'upsample': 2,\n","    'start_block': 0,\n","    'compactness': 50,\n","    'sigma': 0,\n","}\n","dataset_full = FlickrDataset(data_type='flickr30k_c1/val')\n","iou_thr = 0.5\n","model = model(**args).cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SlNWvg7v3wm","executionInfo":{"status":"ok","timestamp":1681655916344,"user_tz":300,"elapsed":26443,"user":{"displayName":"Jay Suh","userId":"06534266656340944442"}},"outputId":"85c1ecb0-13f0-4aeb-e720-f53af0d167aa"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 890M/890M [00:08<00:00, 111MiB/s]\n","/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"]}]},{"cell_type":"code","source":["######### Evaluate the model #########\n","# Randomly select images\n","num_samples = 16\n","idxs = random.sample(range(len(dataset_full)), num_samples)\n","\n","# Create a random subset of the dataset\n","dataset = FlickrDataset(data_type=dataset_full.data_type)\n","dataset.image_paths = [dataset_full.image_paths[idx] for idx in idxs]\n","dataset.bboxes = [dataset_full.bboxes[idx] for idx in idxs]\n","dataset.phrases = [dataset_full.phrases[idx] for idx in idxs]\n","\n","# Lists to hold loaded data\n","imgs = []\n","texts = []\n","bbox_gts = []\n","bbox_preds = []\n","\n","# Predict the bounding boxes\n","for idx in tqdm(range(len(dataset))):\n","\n","    # Data loading - do not call __getitem__ repeatedly\n","    data = dataset[idx] \n","    #print(data['edge_box'])\n","    im = data['image']\n","    text = data['phrases'][0]\n","    bbox_gts.append(data['bbox'])\n","\n","    # Predict\n","    bbox_pred, _ = model(im, text)\n","\n","    # Hold loaded data\n","    imgs.append(im)\n","    texts.append(text)\n","    bbox_preds.append(bbox_pred[0])\n","\n","# Evaluate the model\n","evaluator = GroundingEvaluator(gt_dataset=dataset, iou_thresh=iou_thr)\n","acc = evaluator(torch.from_numpy(np.stack(bbox_preds, axis=0)))\n","print('\\nAcc: {}'.format(acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6Y1Uat_wJ36","executionInfo":{"status":"ok","timestamp":1681658457679,"user_tz":300,"elapsed":73806,"user":{"displayName":"Jay Suh","userId":"06534266656340944442"}},"outputId":"3409efc8-d30d-479c-a8a5-584bb08583b5"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 16/16 [01:13<00:00,  4.58s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Acc: 0.3125\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["######## Visualize multiple images in Grid ########\n","\n","# Set the number of rows and columns in the grid\n","row_num = num_samples // 4 + (1 if num_samples % 4 != 0 else 0)\n","row_num = max(row_num, 2) # at least 2 rows\n","col_num = 4\n","\n","# Red: predicted bounding box\n","# Blue: ground truth bounding box\n","fig, axs = plt.subplots(row_num, col_num, figsize=(20, 20))\n","for i in range(row_num):\n","    for j in range(col_num):\n","\n","        idx = i * col_num + j\n","\n","        # check if the index is out of range\n","        if(idx<num_samples):\n","          im = imgs[idx]\n","          bbox_pred = bbox_preds[idx]\n","          bbox_gt = bbox_gts[idx]\n","\n","          im = cv2.rectangle(im, (int(bbox_pred[0]), int(bbox_pred[1])), (int(bbox_pred[2]), int(bbox_pred[3])), (255, 50, 50), 2)\n","          im = cv2.rectangle(im, (int(bbox_gt[0]), int(bbox_gt[1])), (int(bbox_gt[2]), int(bbox_gt[3])), (50, 50, 255), 2)\n","          \n","          axs[i, j].imshow(im)\n","          axs[i, j].set_title(texts[idx])\n","\n","        # remove the axis\n","        axs[i, j].axis('off')\n","\n","plt.show()\n","\n","# Show the text\n","# print(texts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":865,"output_embedded_package_id":"16ueB5IoRLqHhXQlV9Nhnr08NvDE4HP1m"},"id":"9pGh5AHIwMci","executionInfo":{"status":"ok","timestamp":1681658477306,"user_tz":300,"elapsed":19640,"user":{"displayName":"Jay Suh","userId":"06534266656340944442"}},"outputId":"3106ce34-2bdf-4197-b762-01a06f5ebccf"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"NIk25QqBBrFa"},"execution_count":null,"outputs":[]}]}